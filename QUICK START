
Miniforge3 is available in /apps/miniforge/23.3.1-1
 to create and manage your own python environment. Setup conda using:

  $ source /apps/miniforge/23.3.1-1/etc/profile.d/conda.sh

 or

  $ eval "$(/apps/miniforge/23.3.1-1/bin/conda shell.bash hook)"

 or load the miniforge module

  $ module load miniforge/23.3.1-1

 You will need to create your own environments thereafter.
 e.g. $ conda create --name myPythonInstall
      $ conda activate myPythonInstall

 You can add the bioconda channel with
  $ conda config --add channels bioconda

 Default disk quota for scratch: 1.5TB/user
 Max no of files: 150,000/user

 Default disk quota for /home/users: 200GB/user
 Max no of files: 150,000/user

 To check your current consumption and assigned quota on the pluto login node:
 =============================================================================
 
 For home:
  $ mmlsquota -u $USER --block-size auto fs-hpc:fss-hpc-home

 For scratch:
  $ mmlsquota -u $USER --block-size auto fs-hpc:fss-hpc-scratch

 * your blocks consumption is listed under "blocks" column
 * your files (or inodes) consumption is listed under "files" column
 * your quota is listed under the respective "limit" column


For example,

 for home:
 [$USER@hpc-login01 ~]$ mmlsquota -u $USER --block-size auto fs-hpc:fss-hpc-home
                         Block Limits                                               |     File Limits
 Filesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks
 fs-hpc     fss-hpc-home USR            747M       200G       200G          0     none |      754  150000   150000        0     none esscluster.acrc.a-star.edu.sg

 * the user above consumed 747M out of 200G for block and 754 out of 150000
   for files

 for scratch:
 [$USER@hpc-login01 ~]$ mmlsquota -u $USER --block-size auto fs-hpc:fss-hpc-scratch
                         Block Limits                                               |     File Limits
 Filesystem Fileset    type         blocks      quota      limit   in_doubt    grace |    files   quota    limit in_doubt    grace  Remarks
 fs-hpc     fss-hpc-scratch USR            256K     1.465T     1.465T          0     none |        8  150000   150000        0     none esscluster.acrc.a-star.edu.sg

 * the user above consumed 256K out of 1.465T for block and 8 out of 150000
   for files

 You can run the script /apps/how-to/check-storage-use-and-quota.sh
 to do a quick check on usage.

 Queue info
 ===========

Refer to /apps/how-to/queue.info for job scheduler queue info​
 and /apps/how-to/interacting-with-slurm.txt for basic job scheduler commands.
 
 SLURM job script examples can also be found in /apps/how-to/sample-scripts

 For PBS users, see /apps/how-to/pbs-slurm-translation

 General information for new HPC user
 ====================================
 * Please do not run any jobs on the login node(head node). 
   Any unauthorized process on the login node will be terminated immediately.
 * Please submit your jobs through the job schedulers on each HPC system. 
   Sample job submission scripts are located in /apps/how-to/ on each system.
 * Please use $HOME/scratch to run all jobs and after each job is completed, 
   please copy your important result data to $HOME/DATA or transfer to your
   own storage, e.g. your workstation.
 * Users are strongly advised not to store important data in $SCRATCH, as it
   has no backup.
 * For longer term storage, please store your files in $HOME/DATA.
   This is a common SOD filesystem mounted on all HPC login nodes. This $DATA
   has no backup for itself.
   (NOTE: this space is visible only from the login nodes and not from the
    compute nodes. So users are NOT TO run jobs from DATA.







############################################# how to run interactive queue #######################################################

Update: 02 August 2022

There are now two ways to perform your interactive work in pluto.

1. Use the dedicated interactive queue(new)
This queue is to let you run your workflow and admin stuff.
Typical the stuff you run on your laptop.

2, Request for interactive platform on any of the queues as usual
(except the dedicated interactive queue in point 1 above).
To perform the stuff in point one above in addition
 to be able to test run your programs.

##############################################################################
1. Use the dedicated interactive queue(new)
##############################################################################

There is now a queue dedicated to interactive work.
This is to allow users to perform workflow/administrative 
 work on pluto quickly without using the login node. 
This queue is setup for a maximum of 12 concurrent users. 
One “base” node is dedicated for this queue. 
 
*****************************************************
Partition name: interactive
Nodes: hpc-com112 is dedicated solely to this purpose; 
        thus it is no longer part of any other partitions.
 
Please note the following limits:
Each user can only have one job in this queue at any one time.
Each job can request for up to 4 CPUs, 15G, 8 hours.
 
Please use the following sample command.

  srun -t 8:00:00 --ntasks-per-node 4 --mem=15G -p interactive --pty /bin/bash
 
*Very important note: Please DO NOT specify “--exclusive=user” here. 
This option will reserve the entire node to this job. 
However you will still get the restricted resources(4 Cores, 15GB RAM) but 
 just deprive other users from using this node.
****************************************************** 

##############################################################################
2. Request for interactive platform on any of the queues
##############################################################################

Below is a sample srun command that you can run from the login node.
This will allocate a full node for your interactive session for 1 hour.

  srun -t 1:00:00 -N 1 -p express --ntasks-per-node 1 --cpus-per-task 48 --mem=168G --exclusive=user --pty /bin/bash

to test multithreaded (e.g. OpenMP) applications using 48 cores on the node or

  srun -t 1:00:00 -N 1 -p express --ntasks-per-node 48 --cpus-per-task 1 --mem=168G --exclusive=user --pty /bin/bash

to test MPI jobs within a node. 

Use "-N 2" to test MPI job on 2 nodes (this example will use 96 cores in total).

##############################################################################
##                                                                          ##
## -J = JobName                                                             ##
## -N = number of nodes to select.                                          ##
## --ntasks-per-node = Number of (MPI) tasks to run per node                ##
##                             Max. of 48 cores per node                    ##
## --cpus-per-task = How many (OpenMP) threads on each (MPI) task           ##
##                   Leave this to 1 for pure MPI jobs.                     ##
##                                                                          ##
##                   ntasks-per-node x cpus-per-task should not             ##
##                   exceed 48 cores.                                       ##
##                                                                          ##
## -t = Job time required in HH:MM:SS                                       ##
## --mem-per-cpu = Memory reserved per cpu.                                 ##
##                                                                          ##
##                 Each node has 168,000MB, giving 3,500MB per core.        ##
##                 ntasks-per-node x cpus-per-task x mem-per-cpu            ##
##                 should not exceed 168,000MB.                             ##
##                 Alternatively, total memory per node can be              ##
##                 specified with --mem <MB>                                ##
##                                                                          ##
## -p express -- Select 'express' for higher queue priority in getting an   ##
##               interactive session, upto 1 hour job runtime permitted and ##
##               upto 2 nodes allowed.                                      ##
##                                                                          ##
## --exclusive=user -- add this to reserve the entire allocated node to you ##
##                     if requesting for less than 48 cores per node.       ##
##                                                                          ##
##############################################################################


To test on a GPU node with 1 GPU, use:

  srun -t 1:00:00 -N 1 -p gpuexpress --cpus-per-gpu 6 --mem-per-gpu 21000 --gres=gpu:tesla:1 --pty /bin/bash

You will need to take care not to use more than the number of CPU cores and
memory requested, as they may interfere with the other GPU running jobs.

To test on a GPU nodes with all 8 GPUs, use:

  srun -t 1:00:00 -N 1 -p gpuexpress --cpus-per-gpu 6 --mem-per-gpu 21000 --gres=gpu:tesla:8 --pty /bin/bash


###############################################################################
##                                                                           ##
## #SBATCH -J = JobName                                                      ##
## #SBATCH -p = gpuexpress -- selects the GPU express queue                  ##
## #SBATCH -t = Job time required in HH:MM:SS (upto 1h allowed in gpuexpress) #
## #SBATCH -N = 1 -- GPU jobs are currently restricted to single nodes       ##
## #SBATCH --gres=gpu:tesla:1 -- The number is the requested number of GPUs  ##
##                               There are a maximum of 8 V100 GPUs per node ##
## #SBATCH --cpus-per-gpu = How many cores to reserve per allocated GPU      ##
##                                                                           ##
##                          cpus-per-gpu x gpu:tesla:X should not            ##
##                          exceed 48 cores.                                 ##
##                          There are 48 cores available per node, hence the ##
##                          ideal default is 6 cores per GPU.                ##
##                                                                           ##
## #SBATCH --mem-per-gpu = System memory reserved per allocate GPU.          ##
##                                                                           ##
##                         Each node has 192,000MB, giving 24,000MB per GPU. ##
##                         To allow memory for the OS itself, assume each    ##
##                         node has 168,000MB, giving 21,000MB per GPU.      ##
##                         mem-per-gpu x gpu:tesla:X should not exceed       ##
##                         168,000MB. Alternatively, total memory per node   ##
##                         can be specified with #SBATCH --mem <MB>          ##
##                                                                           ##
## -p gpuexpress -- Select 'gpu express' for higher queue priority in getting #
##                  an interactive session, upto 1 hour job runtime permitted.#
##                                                                           ##
## --exclusive=user -- add this to reserve the entire allocated node to you  ##
##                     if requesting for less than 8 gpus or 48 cpus per node.#
##                                                                           ##
###############################################################################







[lowpm@hpc-login01 how-to]$ cat managing-conda-environments.txt 
**

 NOTICE: As of 09 Oct 2023, A*STAR has instructed all A*STAR staff to halt all new installation
         and usage of Anaconda's software products, including Miniconda, Python with Anaconda and
         PyCharm with Anaconda Plugin.

         The instructions below are valid for miniforge / mambaforge.

**


 Setting up conda environments:
 ------------------------------

 Miniforge3 is available in /apps/miniforge/23.3.1-1
 to create and manage your own python environment. Setup conda using:

  $ source /apps/miniforge/23.3.1-1/etc/profile.d/conda.sh
     or
  $ eval "$(/apps/miniforge/23.3.1-1/bin/conda shell.bash hook)"
     or load the miniforge module
  $ module load miniforge/23.3.1-1

  You will need to create your own environments thereafter.
  e.g. $ conda create --name myPythonInstall
       $ conda activate myPythonInstall

  You can add the bioconda channel with
  $ conda config --add channels bioconda

  DO NOT ADD OR USE CHANNELS FROM https://repo.anaconda.com
  After activating an environment with 'conda activate', check that
  'conda info' does NOT list channel URLs containing
  https://repo.anaconda.com

 A SLURM sample script for job submission can be found at
 /apps/how-to/sample-scripts/sample-miniforge3.sbatch

 To setup mambaforge (Pluto only), execute the following 2 commands:
  $ source /apps/mambaforge/23.1.0-4/etc/profile.d/conda.sh
  $ source /apps/mambaforge/23.1.0-4/etc/profile.d/mamba.sh
     or use the module mambaforge/23.1.0-4
  $ module load mambaforge/23.1.0-4


 Managing your limited disk space:
 ---------------------------------

 To help in managing your limited disk quota, users should do housekeeping
 of their conda installations. 

 We stongly suggest that users clean up cached files when conda environments
 have been setup. To remove all index caches, lock files, unused cache
 packages and tarballs:

  $ conda clean --all

 You can do a dry run to see what will be removed with:

  $ conda clean --all --dry-run

 If disk quota is still an issue, please remove any conda environments that
 will not be used in the future or are redundant. To list the installed
 environments:

  $ conda list

 To delete an environment named 'env_name':

  $ conda remove -n env_name --all

 If disk quota is still an issue, please perform housekeeping of other data
 in home and scratch directories, remove redundant files and decide what
 other data can be moved off the system, either to your own machine or to
 the DATA directory in your home folder.



[lowpm@hpc-login01 sample-scripts]$ cat sample-miniforge3.sbatch 
#!/bin/bash

#SBATCH -J Miniforge_Job
#SBATCH -t 00:05:00
#SBATCH -N 1
#SBATCH --ntasks-per-node 1
#SBATCH --cpus-per-task 48
#SBATCH --mem-per-cpu 3500
#SBATCH --output=%x-%j.out
#SBATCH --error=%x-%j.err
#SBATCH --reservation rexpress

# Setup Miniforge3
module load miniforge/23.3.1-1

# Setup a test environment 'myPythonEnv' with Python 3.10
[ ! -d "$HOME/.conda/envs/myPythonEnv" ] && conda create -y -n myPythonEnv python=3.10

# Activate environment
conda activate myPythonEnv
conda info

# Run python
echo "Python executable is" $(which python)
python --version





























