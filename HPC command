(base) lowpm@IDL-SPL-5HPLQV0 ~ % ssh lowpm@pluto.acrc.a-star.edu.sg
The authenticity of host 'pluto.acrc.a-star.edu.sg (202.83.248.50)' can't be established.
ED25519 key fingerprint is SHA256:a3FpGhiBbrkCf8YKP/Q8y5J+rtDD9mGoswi2QlU0efg.
This key is not known by any other names.
Are you sure you want to continue connecting (yes/no/[fingerprint])? y
Please type 'yes', 'no' or the fingerprint: yes
Warning: Permanently added 'pluto.acrc.a-star.edu.sg' (ED25519) to the list of known hosts.
lowpm@pluto.acrc.a-star.edu.sg's password: 

 *----------------------------------------------------------------------------*

  Welcome to A*CRC HPC System named “pluto”
 
  For new users, please refer to /apps/how-to/QUICKSTART
  For information on using pluto in general, please refer to /apps/how-to/*
  For any queries after reading the above documentation, please email us at
  servicedesk@acrc.a-star.edu.sg
 
  Latest news:
  -----------------
  2nd Jun 2022

  We have a new dedicated interactive queue.
  Before using this queue, please refer to the updated file at 
  /apps/slurm-sample-scripts/interactive-jobs.info
  
  10th Oct 2023

  All staff are instructed to halt all new installation and usage of Anaconda’s software products, including Miniconda, 
  Python with Anaconda and PyCharm with Anaconda Plugin.
  Please refer to broadcast email sent on the 10 Oct 2023 on the subject matter (Use of Anaconda Software).


  A very important reminder to all users
  **************************************
  Please do not run jobs directly on the login node.
  Doing so will make the system unresponsive to everyone.
  This login node is mainly for login and job submission.
  Please use the express queues for quick testing of jobs.

[lowpm@hpc-login01 ~]$ ls
DATA  scratch
[lowpm@hpc-login01 ~]$ sinfo
PARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST 
interactive     up    8:00:00      1    mix hpc-com112 
express         up    1:00:00      1   resv hpc-com007 
express         up    1:00:00     41    mix hpc-com[001,003-006,008,010,012-015,018-021,023,027-028,037,039,044,048-049,059,069,079-080,083-085,087,095,097-103,105-106] 
express         up    1:00:00     53  alloc hpc-com[002,009,011,017,022,024-026,029-030,033,035-036,038,040-043,045-047,052-058,060-065,067,071-078,082,089-092,094,096,104,108-109] 
express         up    1:00:00     16   idle hpc-com[016,031-032,034,050-051,066,068,070,081,086,088,093,107,110-111] 
normal*         up 3-00:00:00      1   resv hpc-com007 
normal*         up 3-00:00:00     41    mix hpc-com[001,003-006,008,010,012-015,018-021,023,027-028,037,039,044,048-049,059,069,079-080,083-085,087,095,097-103,105-106] 
normal*         up 3-00:00:00     53  alloc hpc-com[002,009,011,017,022,024-026,029-030,033,035-036,038,040-043,045-047,052-058,060-065,067,071-078,082,089-092,094,096,104,108-109] 
normal*         up 3-00:00:00     16   idle hpc-com[016,031-032,034,050-051,066,068,070,081,086,088,093,107,110-111] 
long            up 14-00:00:0      1   resv hpc-com007 
long            up 14-00:00:0     41    mix hpc-com[001,003-006,008,010,012-015,018-021,023,027-028,037,039,044,048-049,059,069,079-080,083-085,087,095,097-103,105-106] 
long            up 14-00:00:0     53  alloc hpc-com[002,009,011,017,022,024-026,029-030,033,035-036,038,040-043,045-047,052-058,060-065,067,071-078,082,089-092,094,096,104,108-109] 
long            up 14-00:00:0     16   idle hpc-com[016,031-032,034,050-051,066,068,070,081,086,088,093,107,110-111] 
largemem        up 3-00:00:00      1    mix hpc-lmem01 
largemem        up 3-00:00:00      1  alloc hpc-lmem02 
largememlong    up 14-00:00:0      1    mix hpc-lmem01 
largememlong    up 14-00:00:0      1  alloc hpc-lmem02 
gpuexpress      up    1:00:00      2    mix hpc-accel[01,05] 
gpuexpress      up    1:00:00      3   idle hpc-accel[02-04] 
gpu             up 3-00:00:00      2    mix hpc-accel[01,05] 
gpu             up 3-00:00:00      3   idle hpc-accel[02-04] 
gpulong         up 14-00:00:0      2    mix hpc-accel[01,05] 
gpulong         up 14-00:00:0      3   idle hpc-accel[02-04] 
[lowpm@hpc-login01 ~]$ cd scratch/
[lowpm@hpc-login01 scratch]$ ls
[lowpm@hpc-login01 scratch]$ cd ..
[lowpm@hpc-login01 ~]$ ls
DATA  scratch
[lowpm@hpc-login01 ~]$ cd DATA/
[lowpm@hpc-login01 DATA]$ ls
[lowpm@hpc-login01 DATA]$ cd ..
[lowpm@hpc-login01 ~]$ cd /apps/how-to/
[lowpm@hpc-login01 how-to]$ ls
check-storage-use-and-quota.sh   pbs-slurm-translation  slurm.README
interacting-with-slurm.txt       queue.info             software-list.info
interactive-jobs.info            QUICKSTART
managing-conda-environments.txt  sample-scripts
[lowpm@hpc-login01 how-to]$ 




(base) lowpm@IDL-SPL-5HPLQV0 ~ % scp /Users/lowpm/wes/QC/fastp/bad\ samples/* lowpm@pluto.acrc.a-star.edu.sg:/home/users/astar/aidl/lowpm/scratch/data/bad
lowpm@pluto.acrc.a-star.edu.sg's password: 
CHB7452_L7_1.trim.fq.gz      100% 1044MB   7.2MB/s   02:24    
CHB7452_L7_2.trim.fq.gz      100% 1037MB   7.2MB/s   02:23    
CHB7452_L7.fastp.html        100%  483KB   6.3MB/s   00:00    
CHB7452_L7.fastp.json        100%  136KB   5.5MB/s   00:00    
CHB7452_L8_1.trim.fq.gz      100% 1005MB   7.3MB/s   02:17    
CHB7452_L8_2.trim.fq.gz      100% 1000MB   7.2MB/s   02:19    
CHB7452_L8.fastp.html        100%  484KB   6.8MB/s   00:00    
CHB7452_L8.fastp.json        100%  136KB   5.7MB/s   00:00    
CHB7453_L7_1.trim.fq.gz      100%  935MB   6.9MB/s   02:16    
CHB7453_L7_2.trim.fq.gz      100%  926MB   7.1MB/s   02:09    
CHB7453_L7.fastp.html        100%  484KB   6.8MB/s   00:00    
CHB7453_L7.fastp.json        100%  136KB   5.8MB/s   00:00    
CHB7476_L7_1.trim.fq.gz      100% 1500MB   7.3MB/s   03:25    
CHB7476_L7_2.trim.fq.gz      100% 1495MB   7.3MB/s   03:26    
CHB7476_L7.fastp.html        100%  484KB   7.1MB/s   00:00    
CHB7476_L7.fastp.json        100%  137KB   2.4MB/s   00:00    
CHB7491_L7_1.trim.fq.gz      100% 1090MB   7.3MB/s   02:28    
CHB7491_L7_2.trim.fq.gz      100% 1080MB   7.3MB/s   02:27    
CHB7491_L7.fastp.html        100%  484KB   6.3MB/s   00:00    
CHB7491_L7.fastp.json        100%  136KB   5.4MB/s   00:00    
CHB7491_L8_1.trim.fq.gz      100% 1038MB   7.3MB/s   02:21    
CHB7491_L8_2.trim.fq.gz      100% 1031MB   7.4MB/s   02:19    
CHB7491_L8.fastp.html        100%  484KB   7.1MB/s   00:00    
CHB7491_L8.fastp.json        100%  136KB   6.2MB/s   00:00    
CHB7515_L7_1.trim.fq.gz      100% 1146MB   7.4MB/s   02:34    
CHB7515_L7_2.trim.fq.gz      100% 1133MB   7.3MB/s   02:35    
CHB7515_L7.fastp.html        100%  484KB   7.1MB/s   00:00    
CHB7515_L7.fastp.json        100%  136KB   6.2MB/s   00:00    
CHB7515_L8_1.trim.fq.gz      100% 1092MB   7.0MB/s   02:36    
CHB7515_L8_2.trim.fq.gz      100% 1083MB   6.2MB/s   02:53    
CHB7515_L8.fastp.html        100%  485KB   7.2MB/s   00:00    
CHB7515_L8.fastp.json        100%  137KB   5.4MB/s   00:00    
CHB7532_L7_1.trim.fq.gz      100% 1847MB   7.1MB/s   04:19    
CHB7532_L7_2.trim.fq.gz      100% 1835MB   7.2MB/s   04:15    
CHB7532_L7.fastp.html        100%  485KB   6.2MB/s   00:00    
CHB7532_L7.fastp.json        100%  138KB   6.0MB/s   00:00    
CHB7532_L8_1.trim.fq.gz      100% 1776MB   7.3MB/s   04:03    
CHB7532_L8_2.trim.fq.gz      100% 1769MB   7.4MB/s   03:59    
CHB7532_L8.fastp.html        100%  485KB   7.3MB/s   00:00    
CHB7532_L8.fastp.json        100%  138KB   5.8MB/s   00:00    
CHB7538_L7_1.trim.fq.gz      100%  776MB   7.4MB/s   01:44    
CHB7538_L7_2.trim.fq.gz       28%  221MB   7.6MB/s   01:12




 $ source /apps/miniforge/23.3.1-1/etc/profile.d/conda.sh
     or
  $ eval "$(/apps/miniforge/23.3.1-1/bin/conda shell.bash hook)"
     or load the miniforge module
  $ module load miniforge/23.3.1-1

 You will need to create your own environments thereafter.
  e.g. $ conda create --name myPythonInstall
       $ conda activate myPythonInstall

You can add the bioconda channel with
  $ conda config --add channels bioconda

/apps/how-to/sample-scripts/sample-miniforge3.sbatch





interactive job command:
> srun -t 1:00:00 -N 1 -p express --ntasks-per-node 1 --cpus-per-task 48 --mem=168G --exclusive=user --pty /bin/bash





#!/bin/bash

#SBATCH -J Miniforge_Job
#SBATCH -t 00:05:00
#SBATCH -N 1
#SBATCH --ntasks-per-node 1
#SBATCH --cpus-per-task 48
#SBATCH --mem-per-cpu 3500
#SBATCH --output=%x-%j.out
#SBATCH --error=%x-%j.err
#SBATCH --reservation rexpress

# Setup Miniforge3
module load miniforge/23.3.1-1

# Setup a test environment 'myPythonEnv' with Python 3.10
[ ! -d "$HOME/.conda/envs/myPythonEnv" ] && conda create -y -n myPythonEnv python=3.10

# Activate environment
conda activate myPythonEnv
conda info

# Run python
echo "Python executable is" $(which python)
python --version








